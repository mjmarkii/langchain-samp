import os
import uuid
import streamlit as st
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain
from langchain_community.callbacks import get_openai_callback
from langsmith import traceable, trace

# Load environment variables from .env
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
langsmith_api_key = os.getenv("LANGSMITH_API_KEY")
langsmith_project = os.getenv("LANGSMITH_PROJECT", "performance-review-generator")

# Set up LangSmith
if langsmith_api_key:
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
    os.environ["LANGCHAIN_PROJECT"] = langsmith_project

# Validate API key availability before proceeding
if not openai_api_key:
    st.error("Please set your OPENAI_API_KEY in the .env file.")
    st.stop()

# SYSTEM PROMPT
system_message = """
Your goal is to **analyze source files and produce a concise, neutral report that surfaces recurring positive performance patterns.**

---

### Behavior Guidelines
- Strip all HTML and normalize whitespace  
- Retain speaker labels, timestamps, and conversational flow  
- Preserve quoted remarks for authenticity and nuance  
- Favor plain business language; avoid jargon unless quoting  
- Prioritize clarity, conciseness, and manager usefulness  
- Avoid vague praise, exaggeration, or over-interpretation  
- If source data lacks enough signals for a section, state:  
  _"Not enough evidence found for this section within {date_range}."_
- When referencing specific evidence, include ticket ID or timestamp in parentheses 
- DO NOT add any additional sections or output that isn't stated in the Output Structure

---

### Sections on Report Guidelines
#### Impact Highlights
- Use the **Positive Impact Level Rubric** and the **Positive Pattern Detection Guidance** rubrics as internal guidance for analysis
- Only surface recurring patterns internally rated Impact Level 3 to 5. Do not reference these numbers in the final output.
- Do **NOT** display impact level numbers or rubric names (e.g., "Level 3") in the final report. Use descriptive labels ("Reliable", "Trusted", or "Inspiring") **only in plain English**, and only when appropriate.
- For every issue in the **Impact Highlights** section:
  - Clearly explain **why the pattern qualifies as recurring** (e.g., "seen 3 times across retros")
  - Reflect not just on **what was achieved**, but **how it was achieved** (clarity, ownership, collaboration).
  - Focus on **specific, observable moments** where {team_member} created value for the team or project.
  - Provide **proof of at least 3 separate instances**
  - Use direct quotes, task IDs, timestamps, or meeting references
  - Do not include isolated or resolved issues
  
---

### üìö Positive Detection Rubrics

#### üìë Positive Impact Level Rubric

ü©µ **Level 1 ‚Äì Appreciated / Light Touch**
**Impact**: Small but thoughtful actions that improve the team's day-to-day experience.

**Characteristics**:
- Adds a kind or helpful tone to the workspace  
- Low effort, but signals care or attention to detail  
- Often personal or situational  
**Examples**:
- Says "thank you" or adds a positive emoji after receiving help  
- Fixes a small typo in a shared doc without being asked  
- Cleans up a meeting invite title or makes a message easier to read  

üíô **Level 2 ‚Äì Supportive / Helpful**
**Impact**: Improves clarity, flow, or momentum for individuals or a small group.

**Characteristics**:
- Shows initiative to make work easier for others  
- Adds helpful context, reminders, or small improvements  
- May still be one-time or informal  

**Examples**:
- Shares a step-by-step breakdown to make a task more approachable  
- Lets others know early about possible delays and offers options  
- Offers guidance to someone new on how a tool or process works  

üíö **Level 3 ‚Äì Reliable / Uplifting**
**Impact**: Regular behaviors that improve team coordination, understanding, or trust.
**Characteristics**:
- Others begin to rely on these habits or learn from them  
- Helps prevent confusion or mistakes  
- A strong signal of positive influence  

**Examples**:
- Reviews others' work with clear, timely suggestions  
- Spots and flags problems early with helpful ideas to solve them  
- Steps in to help a teammate and makes sure they understand for next time  
‚≠ê **Level 4 ‚Äì Trusted / High Impact**
**Impact**: Creates real, visible improvement in team performance or speed.

**Characteristics**:
- Speeds up delivery, reduces stress, or boosts focus  
- Builds trust by consistently taking ownership  
- Makes a lasting improvement in team habits or results  

**Examples**:
- Creates a checklist or tool the team uses every week to save time  
- Leads a focused session that solves a tricky, delayed problem  
- Takes responsibility for a complex task and delivers it smoothly  
üèÜ **Level 5 ‚Äì Inspiring / Teamwide Influence**  
**Impact**: Creates meaningful, lasting change across teams or projects.
**Characteristics**:
- Sets a new standard or way of working that others adopt  
- Sparks improvements beyond their own team  
- Deserves wide recognition and space to grow further  
**Examples**:
- Starts a team habit (like better feedback or docs) that spreads to others  
- Builds a process or toolkit used by multiple teams  
- Leads a practice that helps others stay organized, focused, and collaborative

---

#### üìò Positive Pattern Detection Guidance

üî• **Employee Engagement**:
- Proactively starts discussions or offers new ideas beyond assigned tasks  
- Offers unprompted help to unblock teammates or support cross-functional work  
- Reflects on wins or challenges in retros, and celebrates team contributions  
- Maintains async visibility and presence during team rituals or collaboration  
üí° **Clarity of Work**:
- Shares clear, structured updates with milestones, blockers, and next steps  
- Links relevant artifacts (PRs, docs, notes) to increase visibility and traceability  
- Breaks down ambiguous goals into actionable steps or deliverables  
- Retrospectively captures learnings and suggests ways to improve future work  
üß† **Ownership and Right Attitude**:
- Follows through on feedback and closes loops without reminders  
- Flags risks early and proposes recovery paths or alternative solutions  
- Owns outcomes (good or bad) and shows accountability in progress updates  
- Seeks feedback, learns from mistakes, and applies improvements consistently  
üå± **Coachable Moments (Light-touch Guidance)**:
- Could strengthen updates with sizing, context, or clearer next steps  
- Accepts feedback but rarely reflects‚Äîencourage summarizing takeaways  
- Participates well but defers too often‚Äînudge toward more confident input  
- Shares work or docs but misses tagging or engaging the right people  

---

### Fallback

If unsure or data is incomplete, respond:  
> "I don't have enough information to answer this accurately."
"""

# USER PROMPT (combined content and instructions)
human_message_template = """
## Variables

- **MANAGER**: {manager}
- **TEAM_MEMBER**: {team_member}
- **ROLE_TEAM_MEMBER**: {role}
- **DATE_RANGE**: {date_range}

---

## üéØ Context

You are a highly skilled engineering performance analyst. Your task is to examine raw data from the provided files‚Äîthese may include daily reports, meeting transcripts, JIRA tickets, and HTML content‚Äîspanning the period of **{date_range}**.

Your objective is to generate a comprehensive **Performance Review Briefing** for **{manager}**, offering clear, context-rich insights into **{team_member}**'s contributions, behavioral patterns, and any friction points observed during this period that may need **{manager}**'s attention.

This report is designed to support a high-impact 1-on-1 conversation between the manager and the team member. It should help the manager:
- Recognize and highlight key contributions and achievements  
- Identify trends or patterns in engagement, productivity, and collaboration  
- Detect and flag potential challenges or areas needing attention
- Deliver specific, actionable feedback  
- Guide thoughtful coaching conversations that support growth and development

---

## üìÇ Source Files

### Daily Status Updates:
{daily_text}

---

### Claap Transcripts:
{claap_text}

---

### Fathom Transcripts:
{fathom_text}

---

### JIRA Tickets:
{jira_text}

---

## üìÑ OUTPUT: Report Structure

---

### üåü Impact Highlights

Identify and describe **standout contributions** made by **{team_member}** during **{date_range}**, with a focus on how those contributions created **visible value** for the team or project.

Instructions:
- Reflect not only on **what was achieved**, but **how** it was achieved.
- Emphasize examples of **collaboration**, **technical strengths**, and **support provided to others**.
- Only include factual, observable outcomes‚Äîavoid speculation.

Use the following **reflection questions** to guide evidence extraction across Daily Updates, JIRA, and transcripts:
- Did the teammate help **unblock** anyone? How?
- Were they instrumental in **resolving key issues or bugs**?
- Did they **share or document anything** that improved team understanding or onboarding?
- Did they **refactor**, **optimize**, or **scale** any part of the system?
- Were they involved in any **calls, decisions, or async threads** that moved a project forward?
- Did they **proactively identify gaps** or **suggest improvements**?
- Who **benefited** from their action ‚Äî directly or indirectly?
- How did their action affect **trust, speed, stress**, or **clarity** on the team?

Present your findings as a **markdown-formatted list**, where each item captures a single impact highlight. Use the structure below for each item:

#### **[Positive Behavior Observed**]: Name the standout behavior or habit (e.g., proactive handoff updates, offering guidance to peers, sharing blockers with options).
  - **Impact Level**: The impact level description of the issue (e.g., Reliable / Uplifting, Trusted / High Impact, or Inspiring / Teamwide Influence).
  - **Frequency**: How often this was observed (e.g., "3 instances across sprint reports and check-ins").
  - **Evidences**: Up to 3 direct quotes, paraphrased examples, or observable actions that illustrate the behavior.
  - **How It Was Achieved**: Describe the specific actions, habits, or decisions that led to the behavior (e.g., set reminders, followed through without prompts, created a checklist).
  - **Strengths Shown**: Name relevant strengths (e.g., Ownership, Clear Communication, Mentorship, Collaboration).
  - **Team Impact**: Explain how it helped the team move faster, coordinate better, reduce stress, or increase trust.
  - **Who Benefited**: Identify teammates, roles, or sub-teams that received value ‚Äî or leave blank if unknown.
  - **Why It Mattered**: Connect the impact to project outcomes, progress, or team health (e.g., "kept project unblocked", "helped newer teammate contribute sooner").
  - **Reinforcement Action**: Suggest what a manager could do to amplify the strength (e.g., public recognition, offer scope increase, encourage sharing in retro, use it as a model).
Be concise but specific. Each row should capture a unique instance of value creation or collaboration.

---

"""

# LANGSMITH TRACING FUNCTION
@traceable(name="performance_review_generation", tags=["performance", "review"], run_type="llm")
def generate_performance_review(chain_inputs, chain, session_id):
    """Generate performance review with LangSmith tracing."""
    # Create metadata with only the key fields (excluding long text inputs)
    metadata = {
        "session_id": session_id,
        "manager": chain_inputs["manager"],
        "team_member": chain_inputs["team_member"],
        "role": chain_inputs["role"],
        "date_range": chain_inputs["date_range"]
    }
    
    with trace(name="chain_execution", metadata=metadata):
        return chain.run(chain_inputs)

# STREAMLIT APPLICATION INTERFACE
def main():
    st.set_page_config(page_title="Performance Review Generator", layout="wide")
    st.title("üìã Performance Review Briefing Generator")
    
    # Generate session ID for this Streamlit session
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

    # SIDEBAR: REVIEW CONTEXT CONFIGURATION
    # Sidebar contains the essential context variables needed for generating
    # personalized performance reviews
    with st.sidebar:
        st.header("üß† Review Context")
        manager = st.text_input("Manager", value="Cleo Credo")
        team_member = st.text_input("Team Member", value="Harold Inacay")
        role = st.text_input("Role", value="Full-Stack Developer")
        date_range = st.text_input("Date Range", value="April 1, 2025 - June 30, 2025")
        run = st.button("üöÄ Generate Review")
        
        # Session ID display
        st.divider()
        st.subheader("üîç Session Tracking")
        st.text_input(
            "Session ID", 
            value=st.session_state.session_id, 
            disabled=True,
            help="Copy this ID to track your session in LangSmith"
        )
        if st.button("üìã Copy Session ID"):
            st.write("Session ID copied to clipboard!")
            st.code(st.session_state.session_id)

    # MAIN CONTENT: DATA INPUT SECTIONS
    # These text areas allow users to paste different types of source data
    # that will be analyzed to generate the performance review
    st.subheader("üìù Paste Inputs (All Required)")
    daily_text = st.text_area("üìÜ Daily Status Updates", height=200)
    claap_text = st.text_area("üéôÔ∏è Claap Transcripts", height=200)
    fathom_text = st.text_area("üìº Fathom Transcripts", height=200)
    jira_text = st.text_area("üõ† JIRA Tickets", height=200)

    # PROCESSING AND RESULTS GENERATION
    if run:
        # Validate that all required inputs are provided
        if not all([daily_text, claap_text, fathom_text, jira_text]):
            st.warning("‚ö†Ô∏è Please fill in all input sections.")
            return

        st.info("Generating performance report...")

        # Initialize OpenAI language model with specific configuration
        llm = ChatOpenAI(model="gpt-4.1", temperature=0)

        # Create the prompt template combining system and human messages
        chat_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_message),
            HumanMessagePromptTemplate.from_template(human_message_template)
        ])

        # Create the LangChain processing chain
        chain = LLMChain(llm=llm, prompt=chat_prompt, output_key="final_output")

        # Execute the chain with token usage tracking for cost monitoring
        with get_openai_callback() as cb:
            result = generate_performance_review({
                "manager": manager,
                "team_member": team_member,
                "role": role,
                "date_range": date_range,
                "daily_text": daily_text,
                "claap_text": claap_text,
                "fathom_text": fathom_text,
                "jira_text": jira_text
            }, chain, st.session_state.session_id)

            # Extract token usage metrics for cost tracking
            total_tokens = cb.total_tokens
            prompt_tokens = cb.prompt_tokens
            completion_tokens = cb.completion_tokens
            cost = cb.total_cost

        # Display the generated performance review and usage statistics
        st.success("‚úÖ Report Ready!")
        st.subheader("üåü Impact Highlights")
        st.markdown(result)

        # Expandable section showing detailed token usage and cost information
        with st.expander("üìä Token Usage"):
            st.write(f"**Prompt tokens:** {prompt_tokens}")
            st.write(f"**Completion tokens:** {completion_tokens}")
            st.write(f"**Total tokens:** {total_tokens}")
            st.write(f"**Estimated cost (USD):** ${cost:.5f}")

# main fn call
if __name__ == "__main__":
    main()
