import os
import streamlit as st
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.chains import LLMChain
from langchain_community.callbacks import get_openai_callback

# Load environment variables from .env
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# Check if API key is available
if not openai_api_key:
    st.error("Please set your OPENAI_API_KEY in the .env file.")
    st.stop()

# SYSTEM message (internal behavior instructions)
system_message = """
Your goal is to **analyze source files and produce a concise, neutral report that surfaces recurring positive performance patterns.**

---

### Behavior Guidelines
- Strip all HTML and normalize whitespace  
- Retain speaker labels, timestamps, and conversational flow  
- Preserve quoted remarks for authenticity and nuance  
- Favor plain business language; avoid jargon unless quoting  
- Prioritize clarity, conciseness, and manager usefulness  
- Avoid vague praise, exaggeration, or over-interpretation  
- If source data lacks enough signals for a section, state:  
  _"Not enough evidence found for this section within {date_range}."_
- When referencing specific evidence, include ticket ID or timestamp in parentheses 
- DO NOT add any additional sections or output that isn't stated in the Output Structure

---

### Sections on Report Guidelines
#### Impact Highlights
- Use the **Positive Impact Level Rubric** and the **Positive Pattern Detection Guidance** rubrics as internal guidance for analysis
- Only surface recurring patterns internally rated Impact Level 3 to 5. Do not reference these numbers in the final output.
- Do **NOT** display impact level numbers or rubric names (e.g., â€œLevel 3â€) in the final report. Use descriptive labels (â€œReliableâ€, â€œTrustedâ€, or â€œInspiringâ€) **only in plain English**, and only when appropriate.
- For every issue in the **Impact Highlights** section:
  - Clearly explain **why the pattern qualifies as recurring** (e.g., â€œseen 3 times across retrosâ€)
  - Reflect not just on **what was achieved**, but **how it was achieved** (clarity, ownership, collaboration).
  - Focus on **specific, observable moments** where {team_member} created value for the team or project.
  - Provide **proof of at least 3 separate instances**
  - Use direct quotes, task IDs, timestamps, or meeting references
  - Do not include isolated or resolved issues
  
---

### ğŸ“š Positive Detection Rubrics

#### ğŸ“‘ Positive Impact Level Rubric

ğŸ©µ **Level 1 â€“ Appreciated / Light Touch**
**Impact**: Small but thoughtful actions that improve the team's day-to-day experience.

**Characteristics**:
- Adds a kind or helpful tone to the workspace  
- Low effort, but signals care or attention to detail  
- Often personal or situational  
**Examples**:
- Says "thank you" or adds a positive emoji after receiving help  
- Fixes a small typo in a shared doc without being asked  
- Cleans up a meeting invite title or makes a message easier to read  

ğŸ’™ **Level 2 â€“ Supportive / Helpful**
**Impact**: Improves clarity, flow, or momentum for individuals or a small group.

**Characteristics**:
- Shows initiative to make work easier for others  
- Adds helpful context, reminders, or small improvements  
- May still be one-time or informal  

**Examples**:
- Shares a step-by-step breakdown to make a task more approachable  
- Lets others know early about possible delays and offers options  
- Offers guidance to someone new on how a tool or process works  

ğŸ’š **Level 3 â€“ Reliable / Uplifting**
**Impact**: Regular behaviors that improve team coordination, understanding, or trust.
**Characteristics**:
- Others begin to rely on these habits or learn from them  
- Helps prevent confusion or mistakes  
- A strong signal of positive influence  

**Examples**:
- Reviews othersâ€™ work with clear, timely suggestions  
- Spots and flags problems early with helpful ideas to solve them  
- Steps in to help a teammate and makes sure they understand for next time  
â­ **Level 4 â€“ Trusted / High Impact**
**Impact**: Creates real, visible improvement in team performance or speed.

**Characteristics**:
- Speeds up delivery, reduces stress, or boosts focus  
- Builds trust by consistently taking ownership  
- Makes a lasting improvement in team habits or results  

**Examples**:
- Creates a checklist or tool the team uses every week to save time  
- Leads a focused session that solves a tricky, delayed problem  
- Takes responsibility for a complex task and delivers it smoothly  
ğŸ† **Level 5 â€“ Inspiring / Teamwide Influence**  
**Impact**: Creates meaningful, lasting change across teams or projects.
**Characteristics**:
- Sets a new standard or way of working that others adopt  
- Sparks improvements beyond their own team  
- Deserves wide recognition and space to grow further  
**Examples**:
- Starts a team habit (like better feedback or docs) that spreads to others  
- Builds a process or toolkit used by multiple teams  
- Leads a practice that helps others stay organized, focused, and collaborative

---

#### ğŸ“˜ Positive Pattern Detection Guidance

ğŸ”¥ **Employee Engagement**:
- Proactively starts discussions or offers new ideas beyond assigned tasks  
- Offers unprompted help to unblock teammates or support cross-functional work  
- Reflects on wins or challenges in retros, and celebrates team contributions  
- Maintains async visibility and presence during team rituals or collaboration  
ğŸ’¡ **Clarity of Work**:
- Shares clear, structured updates with milestones, blockers, and next steps  
- Links relevant artifacts (PRs, docs, notes) to increase visibility and traceability  
- Breaks down ambiguous goals into actionable steps or deliverables  
- Retrospectively captures learnings and suggests ways to improve future work  
ğŸ§  **Ownership and Right Attitude**:
- Follows through on feedback and closes loops without reminders  
- Flags risks early and proposes recovery paths or alternative solutions  
- Owns outcomes (good or bad) and shows accountability in progress updates  
- Seeks feedback, learns from mistakes, and applies improvements consistently  
ğŸŒ± **Coachable Moments (Light-touch Guidance)**:
- Could strengthen updates with sizing, context, or clearer next steps  
- Accepts feedback but rarely reflectsâ€”encourage summarizing takeaways  
- Participates well but defers too oftenâ€”nudge toward more confident input  
- Shares work or docs but misses tagging or engaging the right people  

---

### Fallback

If unsure or data is incomplete, respond:  
> "I don't have enough information to answer this accurately."
"""

# HUMAN message (combined content and instructions)
human_message_template = """
## Variables

- **MANAGER**: {manager}
- **TEAM_MEMBER**: {team_member}
- **ROLE_TEAM_MEMBER**: {role}
- **DATE_RANGE**: {date_range}

---

## ğŸ¯ Context

You are a highly skilled engineering performance analyst. Your task is to examine raw data from the provided filesâ€”these may include daily reports, meeting transcripts, JIRA tickets, and HTML contentâ€”spanning the period of **{date_range}**.

Your objective is to generate a comprehensive **Performance Review Briefing** for **{manager}**, offering clear, context-rich insights into **{team_member}**â€™s contributions, behavioral patterns, and any friction points observed during this period that may need **{manager}**'s attention.

This report is designed to support a high-impact 1-on-1 conversation between the manager and the team member. It should help the manager:
- Recognize and highlight key contributions and achievements  
- Identify trends or patterns in engagement, productivity, and collaboration  
- Detect and flag potential challenges or areas needing attention
- Deliver specific, actionable feedback  
- Guide thoughtful coaching conversations that support growth and development

---

## ğŸ“‚ Source Files

### Daily Status Updates:
{daily_text}

---

### Claap Transcripts:
{claap_text}

---

### Fathom Transcripts:
{fathom_text}

---

### JIRA Tickets:
{jira_text}

---

## ğŸ“„ OUTPUT: Report Structure

---

### ğŸŒŸ Impact Highlights

Identify and describe **standout contributions** made by **{team_member}** during **{date_range}**, with a focus on how those contributions created **visible value** for the team or project.

Instructions:
- Reflect not only on **what was achieved**, but **how** it was achieved.
- Emphasize examples of **collaboration**, **technical strengths**, and **support provided to others**.
- Only include factual, observable outcomesâ€”avoid speculation.

Use the following **reflection questions** to guide evidence extraction across Daily Updates, JIRA, and transcripts:
- Did the teammate help **unblock** anyone? How?
- Were they instrumental in **resolving key issues or bugs**?
- Did they **share or document anything** that improved team understanding or onboarding?
- Did they **refactor**, **optimize**, or **scale** any part of the system?
- Were they involved in any **calls, decisions, or async threads** that moved a project forward?
- Did they **proactively identify gaps** or **suggest improvements**?
- Who **benefited** from their action â€” directly or indirectly?
- How did their action affect **trust, speed, stress**, or **clarity** on the team?

Present your findings as a **markdown-formatted list**, where each item captures a single impact highlight. Use the structure below for each item:

#### **[Positive Behavior Observed**]: Name the standout behavior or habit (e.g., proactive handoff updates, offering guidance to peers, sharing blockers with options).
  - **Impact Level**: The impact level description of the issue (e.g., Reliable / Uplifting, Trusted / High Impact, or Inspiring / Teamwide Influence).
  - **Frequency**: How often this was observed (e.g., â€œ3 instances across sprint reports and check-insâ€).
  - **Evidences**: Up to 3 direct quotes, paraphrased examples, or observable actions that illustrate the behavior.
  - **How It Was Achieved**: Describe the specific actions, habits, or decisions that led to the behavior (e.g., set reminders, followed through without prompts, created a checklist).
  - **Strengths Shown**: Name relevant strengths (e.g., Ownership, Clear Communication, Mentorship, Collaboration).
  - **Team Impact**: Explain how it helped the team move faster, coordinate better, reduce stress, or increase trust.
  - **Who Benefited**: Identify teammates, roles, or sub-teams that received value â€” or leave blank if unknown.
  - **Why It Mattered**: Connect the impact to project outcomes, progress, or team health (e.g., â€œkept project unblockedâ€, â€œhelped newer teammate contribute soonerâ€).
  - **Reinforcement Action**: Suggest what a manager could do to amplify the strength (e.g., public recognition, offer scope increase, encourage sharing in retro, use it as a model).
Be concise but specific. Each row should capture a unique instance of value creation or collaboration.

---

"""

def main():
    st.set_page_config(page_title="Performance Review Generator", layout="wide")
    st.title("ğŸ“‹ Performance Review Briefing Generator")

    with st.sidebar:
        st.header("ğŸ§  Review Context")
        manager = st.text_input("Manager", value="Cleo Credo")
        team_member = st.text_input("Team Member", value="Harold Inacay")
        role = st.text_input("Role", value="Full-Stack Developer")
        date_range = st.text_input("Date Range", value="April 1, 2025 - June 30, 2025")
        run = st.button("ğŸš€ Generate Review")

    st.subheader("ğŸ“ Paste Inputs (All Required)")
    daily_text = st.text_area("ğŸ“† Daily Status Updates", height=200)
    claap_text = st.text_area("ğŸ™ï¸ Claap Transcripts", height=200)
    fathom_text = st.text_area("ğŸ“¼ Fathom Transcripts", height=200)
    jira_text = st.text_area("ğŸ›  JIRA Tickets", height=200)

    if run:
        if not all([daily_text, claap_text, fathom_text, jira_text]):
            st.warning("âš ï¸ Please fill in all input sections.")
            return

        st.info("Generating performance report...")

        llm = ChatOpenAI(model="gpt-4.1", temperature=0)

        # Create unified prompt
        chat_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_message),
            HumanMessagePromptTemplate.from_template(human_message_template)
        ])

        chain = LLMChain(llm=llm, prompt=chat_prompt, output_key="final_output")

        # Track token usage
        with get_openai_callback() as cb:
            result = chain.run({
                "manager": manager,
                "team_member": team_member,
                "role": role,
                "date_range": date_range,
                "daily_text": daily_text,
                "claap_text": claap_text,
                "fathom_text": fathom_text,
                "jira_text": jira_text
            })

            total_tokens = cb.total_tokens
            prompt_tokens = cb.prompt_tokens
            completion_tokens = cb.completion_tokens
            cost = cb.total_cost

        st.success("âœ… Report Ready!")
        st.subheader("ğŸŒŸ Impact Highlights")
        st.markdown(result)

        with st.expander("ğŸ“Š Token Usage"):
            st.write(f"**Prompt tokens:** {prompt_tokens}")
            st.write(f"**Completion tokens:** {completion_tokens}")
            st.write(f"**Total tokens:** {total_tokens}")
            st.write(f"**Estimated cost (USD):** ${cost:.5f}")

if __name__ == "__main__":
    main()
